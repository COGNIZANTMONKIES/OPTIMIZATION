https://en.wikipedia.org/wiki/Algorithmic_efficiency

algorithmic efficiency:
  a property of an algorithm which relates to the number of computational resources
  used by the algorithm. An algorithm must be analyzed to determmine its resource usage, 
  and the efficiency of an algorithm can be measured based on usage of different resources.
  Algorithmic efficiency can be thought of as analogous to engineering productivity
  for a repeating or continuous process.
 
conditional approach weights value system:
  For maximum efficiency we wish to minimize resource usage. however, different resources
  such as time and space complexity cannot be compared directly, so which of two algorithms
  is considered to be more efficient often depends on which measure of efficiency is considered
  most important.
  
evident algorithmic optimization grassroots:
  early electronic computers were severely limited both by the speed of operations and the
  amount of memory available. In some cases it was realized there was a space-time trade-off,
  whereby a task could be handled either by using a fast algorithm which used quite a lot of
  working memory, or by using a slower algorithm which used very little working memory.
  The engineering tradeoff was then to use the fastest algorithm which would fit in the available
  memory. Modern computers are significantly faster than early computers, and have a much larger
  amount of memory available. Nevertheless, Donald Knuth emphasised that efficiency is still an 
  important consideration "in established engineering disciplines a 12% improvement,
  easily obtained, is never considered marginal and I believe the same viewpoint should prevail
  in software engineering"

known ways of measuring efficiency:
  the two most common ways measures are speed and memory usage;
  other measures could include transmission speed, temporary disk usage,
  long-term disk usage, power consumption, total cost of ownership,
  response time to external stimuli, etc. Many of these measures depend 
  on size of input to the algorithm, i.e. the amount of data to be processed.
  They might also depend on the way in which the data is arranged; for example,
  some sorting algorithms perform poorly on data which is already sorted,
  or which is sorted in reverse order. In practice, there are other factors
  which affect the efficiency of an algorithm, such as requirements for accuracy
  and/or reliability. As detailed below, the way in which an algorithm is implemented
  can also have a significant effect on actual efficiency, though many aspects of this
  relate to optimization issues.
  
theoretical analysis
  the normal practice is to estimate their complexity in the asymptotic sense.
  most commonly used notation to describe resource consumption or "complexity"
  is Donald Knuths Big O notation, representing the complexity of an algorithm as a 
  function of the size of the input. Big O notation is an asymptotic measure of function
  complexity, where f(n) = O(g(n)) roughly means the time requirement for an algorithm
  is proportional to y(n), omitting lower-order terms that contribute less than g(n) to the 
  growth of the function as n grows arbitrarily large. This estimate may be misleading
  when n is small, but is generally sufficiently accurate when n is large as the notation
  is asymptotic.
  
  O(1)        constant
  O(log n)    logarithmic
  O(n)        linear
  O(n log n)  linearithmic, loglinear, or quasilinear
  O(n**2)     quadratic
  O(c**n),c>1 exponential
  
  
Benchmarking (measuring performance):
  
  for new versions of software or to provide comparisons with competitive systems,
  benchmarks are sometime used, which assist with gauging an algorithms relative
  performance. If a new sort algorithm is produced, for example, it can be compared
  with it's predecessors to ensure that at least it is efficient as before with known data,
  taking into consideration any functional improvements.


Implementation concerns:
  
  beware of interpretted languages as they are the slowest to execute
  versus a compiler
  there are other factors which may affect time or space issues, but which
  may be outside of a programmers control; these include data alignment,
  data granularity, cache locality, cache coherency, garbage collection,
  instruction-level parallelism, multi-threading (at either a hardware or software
  level), simulataneous multitasking, and subroutine calls.
  
Measures of resource usage:
  - time
  - space
  - direct power consumption
  - indirect power consumption
  - transmission size
  - external space
  - response time
  - total cost of onwership
  
space:
  there are 4 aspects of memory usage to consider:
  - amount of memory needed to hold the code for the algorithm
  - amount of memory needed for the input data
  - amount of memory needed for the output data
  




  
